<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="description"
          content="Gener Project: A Collection of Genomic Foundation Models.">
    <meta name="keywords" content="GENERator, GENERanno, Genomic, DNA Language Model">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>Gener Project: A Collection of Genomic Foundation Models</title>

    <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
    <link rel="stylesheet" href="./static/css/bulma.min.css">
    <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
    <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
    <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link rel="stylesheet" href="./static/css/index.css">
    <link rel="icon" href="./static/images/favicon.ico">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
    <script defer src="./static/js/fontawesome.all.min.js"></script>
    <script src="./static/js/bulma-carousel.min.js"></script>
    <script src="./static/js/bulma-slider.min.js"></script>
    <script src="./static/js/index.js"></script>

    <style>
        .is-size-5 {
            font-size: 1.25rem;
            line-height: 1.5;
        }

        .indent {
            margin-left: 2rem;
        }
    </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
    <div class="navbar-brand">
        <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
        </a>
    </div>
    <div class="navbar-menu">
        <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
            <a class="navbar-item" href="#">
                <span class="icon">
                  <i class="fas fa-home"></i>
                </span>
            </a>
            <div class="navbar-item has-dropdown is-hoverable">
                <a class="navbar-link">
                    Our Works
                </a>
                <div class="navbar-dropdown">
                    <a class="navbar-item" href="https://github.com/GenerTeam/GENERator">
                        <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">ator</span>
                    </a>
                    <a class="navbar-item" href="https://github.com/GenerTeam/GENERanno">
                        <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">anno</span>
                    </a>
                </div>
            </div>
        </div>
    </div>
</nav>

<section class="hero">
    <div class="hero-body">
        <div class="container is-max-desktop">
            <div class="columns is-centered">
                <div class="column has-text-centered">
                    <h1 class="title is-1 publication-title">Gener Project</h1>
                    <p class="is-size-5">
                        We are part of the AI for Science team at Apsara Lab, Alibaba Cloud, <br>
                        with a primary focus on genomic analysisâ€”hence our nickname, the <strong>Geners</strong>.
                    </p>
                    <div class="contact-links">
                        <span class="link-block">
                          <a href="https://github.com/GenerTeam"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon"><i class="fab fa-github"></i></span>
                            <span>Github</span>
                          </a>
                        </span>
                        <span class="link-block">
                          <a href="https://huggingface.co/GenerTeam"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon" style="font-size:18px">ðŸ¤—</span>
                            <span>HuggingFace</span>
                          </a>
                        </span>
                        <span class="link-block">
                          <a href="https://arxiv.org/abs/2502.07272"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon"><i class="ai ai-arxiv"></i></span>
                            <span>arXiv</span>
                          </a>
                        </span>
                        <span class="link-block">
                          <a href="mailto:generteam2024@gmail.com"
                             class="external-link button is-normal is-rounded is-dark">
                            <span class="icon"><i class="fas fa-envelope"></i></span>
                            <span>Email</span>
                          </a>
                        </span>
                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

<!-- Teaser Section -->
<section class="hero teaser">
    <div class="container is-max-desktop">
        <div class="hero-body">
            <p class="is-size-5">
                The Gener Project currently features two distinct model lines:
            </p>
            <p class="is-size-5 indent">
                <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">ator</span>: A suite of
                generative genomic foundation models focusing on <strong>DNA sequence design</strong>.
            </p>
            <p class="is-size-5 indent">
                <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">anno</span>: A collection
                of genomic foundation models dedicated to <strong>gene annotation</strong>.
            </p>
            <p class="is-size-5">
                <br>We are committed to promoting transparency and collaboration in research. All necessary materials to
                replicate our workâ€”including data, code, and model weightsâ€”will be fully open-sourced on the GenerTeam
                GitHub and HuggingFace pages. We hope that this contribution will help advance the development of the
                <strong>DNA language model community</strong>.
            </p>
            <p class="is-size-5">
                <br>If you are interested in our work, have insightful feedback, or wish to explore collaboration
                opportunities, please contact us at <a href="mailto:generteam2024@gmail.com">generteam2024@gmail.com</a>.
            </p>
        </div>
    </div>
</section>

<!-- GENERator Section -->
<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3"><strong>Gener</strong><span style="font-style: italic; font-weight: normal;">ator</span>
        </h2>
        <h3 class="title is-4">Overview</h3>
        <div class="content has-text-justified">
            <p>
                In this study, we introduced <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span>, a collection of generative genomic
                foundation models utilizing the transformer decoder architecture, trained on expansive DNA datasets
                derived from the <a href="https://www.ncbi.nlm.nih.gov/refseq/">RefSeq database</a>. Our evaluations
                demonstrate that the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> consistently achieves state-of-the-art
                performance across a wide spectrum of benchmarks, including <a
                    href="https://huggingface.co/datasets/katielink/genomic-benchmarks/tree/main">Genomic Benchmarks</a>,
                <a href="https://huggingface.co/datasets/InstaDeepAI/nucleotide_transformer_downstream_tasks_revised">NT
                    tasks</a>, and our newly proposed <a href="https://huggingface.co/GenerTeam">Gener tasks</a>.
            </p>
            <p>
                Beyond benchmark performance, the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> adheres to the <strong>central
                dogma</strong> of molecular biology, accurately generating protein-coding DNA sequences that produce
                proteins structurally analogous to known families. Moreover, the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> showcases significant promise in
                sequence optimization, particularly in the design of promoter sequences that regulate gene activity
                during various biological stages. Our findings position the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> as a vital resource for genomic
                research and biotechnological advancement. By enhancing our ability to interpret and predict genomic
                sequences, the <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">ator</span>
                paves the way for profound improvements in our understanding of complex biological systems and the
                development of precise genomic interventions. For more technical details, please refer to our paper <a
                    href="https://arxiv.org/abs/2502.07272">"GENERator: A Long-Context Generative Genomic Foundation
                Model"</a>.
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/model_overview.png" width="800" alt="Model Overview">
        </div>
        <br>
        <h3 class="title is-4">Data Preparation</h3>
        <div class="content has-text-justified">
            <p>
                For training the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> (<a
                    href="https://huggingface.co/GenerTeam/GENERator-eukaryote-1.2b-base">GENERator-eukaryote-1.2b-base</a>),
                we explored two data processing strategies:
            </p>
            <p>
                <strong>Gene Sequence Training:</strong> Utilizing the rich annotation data available in RefSeq, we
                isolated gene regions from genomic sequences. These regions encompass a wide array of functionalities,
                including transcription into various RNA molecules, translation into complex proteins, and regulatory
                functions such as promoters and enhancers that control gene expression. Defined broadly as gene regions,
                these biologically functional DNA segments formed our training samples, totaling 386B nucleotides.
            </p>
            <p>
                <strong>Whole Sequence Training:</strong> In this approach, we fed a mixture of gene and non-gene DNA
                sequences from all eukaryotic organisms in RefSeq directly into the language model for training. This
                dataset includes approximately 2T nucleotides. This strategy aligns with the conventional pre-training
                paradigm for general LLMs and forms the foundational training approach seen in existing DNA language
                models.
            </p>
            <p>
                Generally, while Scheme 2 displays a lower pre-training loss, Scheme 1 consistently outperforms Scheme 2
                across a range of downstream tasksâ€”even for tasks where non-gene context dominates. One possible
                explanation for this counterintuitive result is the inherent difference between DNA and human language.
                DNA is not a concise language but is filled with <strong>randomness</strong> and
                <strong>redundancy</strong>. Over billions of years of evolution, random processes have given rise to a
                limited number of biologically functional sequences, which represent the "semantics" of DNA.
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/pretrain_loss.png" width="400" alt="Pre-training Loss">
        </div>
        <br>
        <h3 class="title is-4">Tokenization</h3>
        <div class="content has-text-justified">
            <p>
                The selection of an appropriate tokenizer involves a trade-off between sequence resolution and
                contextual coverage. The <strong>single nucleotide tokenizer</strong> provides the highest resolution,
                capturing details at the finest level, but for a fixed number of tokens, this increased granularity
                results in a reduced context window compared to K-mer or BPE tokenizers. Additionally, the computational
                cost associated with the attention mechanism increases quadratically with sequence length.
            </p>
            <p>
                Earlier studies such as <a href="https://arxiv.org/abs/2306.15006">DNABERT-2</a> and <a
                    href="https://www.nature.com/articles/s42256-024-00872-0">GROVER</a> demonstrated that the <strong>BPE
                tokenizer</strong> is optimal for masked language models. However, our experiments indicate that in the
                causal language model context, the <strong>K-mer tokenizer (6-mer)</strong> significantly outperforms
                other approaches in next token prediction (NTP) pre-training. This observation further underscores the
                intrinsic differences between DNA sequences and human language, which lack clearly defined lexical
                boundaries.
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/kmer_prediction.png" width="400" alt="K-mer Prediction">
        </div>
        <br>
        <h3 class="title is-4">Experiments</h3>
        <h4 class="title is-5">Benchmark Evaluations</h4>
        <div class="content has-text-justified">
            <p>
                Both Gener models, <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> and <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">anno</span>, significantly outperform baseline
                models across a wide range of benchmarks, establishing them as the top genomic foundation models in the
                field (2025-02).
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/benchmarks.png" width="800" alt="Benchmark Evaluations">
        </div>

        <h4 class="title is-5">Central Dogma</h4>
        <div class="content has-text-justified">
            <p>
                We fine-tuned the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> model to generate protein-coding DNA
                sequences. The distribution of the generated sequences closely resembles that of the natural family
                (Cytochrome P450). We further assessed whether protein language models "recognize" these generated
                protein sequences by calculating their perplexity (PPL) using <a
                    href="https://arxiv.org/abs/2206.13517">ProGen2</a>. The results show that the PPL distribution of
                generated sequences closely matches that of natural families and significantly differs from that of
                shuffled sequences. Furthermore, we used <a href="https://www.nature.com/articles/s41586-024-07487-w">AlphaFold3</a>
                to predict the folded structures of the generated protein sequences and employed Foldseek to find
                analogous proteins in the Protein Data Bank (PDB). Remarkably, we identified numerous instances where
                the conformations of the generated sequences exhibited high similarity to established structures.
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/cytochrome_generation.png" width="400" alt="Cytochrome Generation">
        </div>

        <h4 class="title is-5">Promoter Design</h4>
        <div class="content has-text-justified">
            <p>
                We developed a promoter activity predictor by fine-tuning the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span>, employing promoter activity data from
                <a href="https://www.nature.com/articles/s41588-022-01048-5">DeepSTARR</a>. This predictor surpasses the
                accuracy of DeepSTARR and NT-multi, establishing itself as the current state-of-the-art predictor. By
                selecting promoter sequences from the top and bottom quartiles of activity values and labeling them with
                the prompts &lt;<strong>high</strong>&gt; and &lt;<strong>low</strong>&gt;, our refined fine-tuning
                approach enables the design of promoter sequences with desired activity profiles. The predicted
                activities of these sequences exhibit significant differentiation compared to natural samples.
            </p>
        </div>
        <div style="text-align: center;">
            <img src="./static/images/promoter_design.png" width="400" alt="Promoter Design">
        </div>
    </div>
</section>

<!-- GENERanno Section -->
<section class="section">
    <div class="container is-max-desktop">
        <h2 class="title is-3"><strong>Gener</strong><span style="font-style: italic; font-weight: normal;">anno</span>
        </h2>
        <div class="content has-text-justified">
            <p>
                In this study, we introduced <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">anno</span>, a collection of genomic foundation
                models utilizing the transformer encoder architecture, trained on expansive DNA datasets derived from
                the <a href="https://www.ncbi.nlm.nih.gov/refseq/">RefSeq database</a>. Our evaluations demonstrate that
                <strong>Gener</strong><span style="font-style: italic; font-weight: normal;">anno</span> achieves
                comparable performance with <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">ator</span> in benchmark evaluations, establishing
                them as the top genomic foundation models in the field (2025-02).
            </p>
            <p>
                Beyond benchmark performance, the <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">anno</span> model is meticulously designed with a
                specialization in <strong>gene annotation</strong>. The model efficiently and accurately identifies gene
                locations, predicts gene functions, and annotates gene structureâ€”highlighting its potential to
                revolutionize genomic research by significantly enhancing the precision and efficiency of gene
                annotation processes.
            </p>
            <p>
                Please note that <strong>Gener</strong><span
                    style="font-style: italic; font-weight: normal;">anno</span> is currently in the developmental
                phase. We are actively refining the model and will release more technical details soon. <strong>Stay
                tuned for updates!</strong>
            </p>
        </div>
    </div>
</section>

<!-- Citation Section -->
<section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
        <h2 class="title">Citation</h2>
        <pre><code>@misc{wu2025generator,
    title={GENERator: A Long-Context Generative Genomic Foundation Model},
    author={Wei Wu and Qiuyi Li and Mingyang Li and Kun Fu and Fuli Feng and Jieping Ye and Hui Xiong and Zheng Wang},
    year={2025},
    eprint={2502.07272},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2502.07272},
}</code></pre>
    </div>
</section>

</body>
</html>
